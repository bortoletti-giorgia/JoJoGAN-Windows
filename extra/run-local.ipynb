{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/prodramp/DeepWorks/tree/main/JoJoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6MfoD_zOVxP"
   },
   "source": [
    "## Setup Anaconda env for JoJoGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!git clone https://github.com/mchong6/JoJoGAN.git\n",
    "\n",
    "conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=10.2 -c pytorch\n",
    "!pip install tqdm gdown scikit-learn==0.22 scipy lpips dlib==19.20 opencv-python wandb matplotlib scikit-image pybind11 cmake ninja\n",
    "!conda install -c conda-forge ffmpeg'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tPx_mhrOj_-"
   },
   "source": [
    "# Importing required Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gsyBOLdV-7t6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giorgia\\anaconda3\\envs\\jojo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn, autograd, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huSPAEKLOogc"
   },
   "source": [
    "## Setting Cudnn benchmark usage for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDdJDnPXLltx"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Y80KCK7OvUw"
   },
   "source": [
    "## Importing JoJoGAN Specific Python Modules\n",
    "- These modules are loaded from the JoJoGAN GitHub repo code (local file system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \\JoJoGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKa7MyqOLo27"
   },
   "outputs": [],
   "source": [
    "# JoJoGAN Specific Import\n",
    "from model import *\n",
    "from e4e_projection import projection as e4e_projection\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhbDIpXTmUHP"
   },
   "source": [
    "## Python DeepCopy\n",
    "The difference between shallow and deep copying is only relevant for compound objects (objects that contain other objects, like lists or class instances):\n",
    "\n",
    "- A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.\n",
    "- A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-yjnw96Mfot"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_Sw8a3NO68f"
   },
   "source": [
    "## Creating local folders for local content creation and management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NJp6JXZ_EaV"
   },
   "outputs": [],
   "source": [
    "os.makedirs('inversion_codes', exist_ok=True)\n",
    "os.makedirs('style_images', exist_ok=True)\n",
    "os.makedirs('style_images_aligned', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DR2MODuZPBzB"
   },
   "source": [
    "# Downloading 68 Shape Predictor DLIB Model to read faces from the source images\n",
    "- shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmE31otO_H1A",
    "outputId": "0fa8cd22-d4d1-4f54-e39a-ee33a268946f"
   },
   "outputs": [],
   "source": [
    "# download http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "# export shape_predictor_68_face_landmarks.dat in models/ with name dlibshape_predictor_68_face_landmarks.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7apE_LsPNVl"
   },
   "source": [
    "## Various Face Style JoJoGAN specific prebuilt models definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EQFGpEAMOCy"
   },
   "source": [
    "# Downloading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GUcs1UC_OJo"
   },
   "outputs": [],
   "source": [
    "drive_ids = {\n",
    "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
    "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
    "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
    "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
    "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
    "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
    "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
    "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
    "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
    "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
    "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
    "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
    "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
    "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
    "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
    "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
    "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "850JdZsUC1_M"
   },
   "outputs": [],
   "source": [
    "# from StyelGAN-NADA\n",
    "class Downloader(object):\n",
    "    def download_file(self, file_name):\n",
    "        file_dst = os.path.join('models', file_name)\n",
    "        file_id = drive_ids[file_name]\n",
    "        if not os.path.exists(file_dst):\n",
    "            print(f'Downloading {file_name}')\n",
    "            !gdown --id $file_id -O $file_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcpcdOOwME3T",
    "outputId": "0f7433fa-6a4c-4d87-bfd0-1609ce478896"
   },
   "outputs": [],
   "source": [
    "downloader = Downloader()\n",
    "#downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
    "#downloader.download_file('e4e_ffhq_encode.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsbw2iUxPZSu"
   },
   "source": [
    "## Making sure to use CUDA as GPU device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNMprAbuPyFK"
   },
   "source": [
    "## Loading StyleGAN model\n",
    "- Already trained using FFHQ dataset - 70K faces\n",
    "- https://github.com/NVlabs/ffhq-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZYENyq2MBIv"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' #@param ['cuda', 'cpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpY8qirH_ZGU"
   },
   "outputs": [],
   "source": [
    "latent_dim = 512\n",
    "\n",
    "# Load original generator\n",
    "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
    "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
    "#ckpt = torch.load('C:/0_thesis/1_stylegan/from-official-to-rosinality/test.pt', map_location=lambda storage, loc: storage)\n",
    "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
    "mean_latent = original_generator.mean_latent(10000)\n",
    "\n",
    "# to be finetuned generator\n",
    "generator = deepcopy(original_generator)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((1024, 1024)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fMiT0WmQJZx"
   },
   "source": [
    "## Source Image Setup to extract Face\n",
    "- Extract Face from source\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "y1CMrZCbB5T7",
    "outputId": "12c35568-36eb-480c-9dcc-34c7bdd1167c"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "#@title Choose input face\n",
    "#@markdown Add your own image to the test_input directory and put the name here\n",
    "#filename = 'iu.jpeg' #@param {type:\"string\"}\n",
    "\n",
    "#filename = 'nm0000101_rm807848704_1952-7-1_1989.jpg' #@param {type:\"string\"}\n",
    "filename = 'old.jpg' #@param {type:\"string\"}\n",
    "\n",
    "filepath = f'test_input/{filename}'\n",
    "\n",
    "\n",
    "# uploaded = files.upload()\n",
    "# filepath = list(uploaded.keys())[0]\n",
    "name = strip_path_extension(filepath)+'.pt'\n",
    "\n",
    "# aligns and crops face from the source image\n",
    "aligned_face = align_face(filepath)\n",
    "\n",
    "# my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
    "my_w = e4e_projection(aligned_face, name, device).unsqueeze(0)\n",
    "\n",
    "display_image(aligned_face, title='Aligned face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gm8yId8MzMC",
    "outputId": "540339ba-3bd8-4a8e-e63a-73875d20951e"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "pretrained = 'art' #@param ['art', 'arcane_multi', 'sketch_multi', 'arcane_jinx', 'arcane_caitlyn', 'jojo_yasuho', 'jojo', 'disney']\n",
    "#@markdown Preserve color tries to preserve color of original image by limiting family of allowable transformations. Otherwise, the stylized image will inherit the colors of the reference images, leading to heavier stylizations.\n",
    "preserve_color = False #@param{type:\"boolean\"}\n",
    "\n",
    "if preserve_color:\n",
    "    ckpt = f'{pretrained}_preserve_color.pt'\n",
    "else:\n",
    "    ckpt = f'{pretrained}.pt'\n",
    "\n",
    "# load base version if preserve_color version not available\n",
    "try:\n",
    "    downloader.download_file(ckpt)\n",
    "except:\n",
    "    ckpt = f'{pretrained}.pt'\n",
    "    downloader.download_file(ckpt)\n",
    "\n",
    "#@title Generate results\n",
    "n_sample =  5#@param {type:\"number\"}\n",
    "seed = 3000 #@param {type:\"number\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVF8ZQmhNnaQ",
    "outputId": "c388ad98-e318-47c2-81ea-31248965aeb2"
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(os.path.join('models', ckpt), map_location=lambda storage, loc: storage)\n",
    "generator.load_state_dict(ckpt[\"g\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JO6T4FJXngOh",
    "outputId": "48b09fe6-25b9-4445-e6b5-54a81b267eac"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "with torch.no_grad():\n",
    "    generator.eval()\n",
    "    z = torch.randn(n_sample, latent_dim, device=device)\n",
    "\n",
    "    #original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
    "    #sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
    "\n",
    "    #original_my_sample = original_generator(my_w, input_is_latent=True)\n",
    "    my_sample = generator(my_w, input_is_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWIYDRF7NxMN"
   },
   "outputs": [],
   "source": [
    "# display reference images\n",
    "if pretrained == 'arcane_multi':\n",
    "    style_path = f'style_images_aligned/arcane_jinx.png'\n",
    "elif pretrained == 'sketch_multi':\n",
    "    style_path = f'style_images_aligned/sketch.png'\n",
    "else:   \n",
    "    style_path = f'style_images_aligned/{pretrained}.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MEIpPx4N0K3"
   },
   "outputs": [],
   "source": [
    "style_image = transform(Image.open(style_path)).unsqueeze(0).to(device)\n",
    "face = transform(aligned_face).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "KRy2LyVkN3hV",
    "outputId": "945a37d5-6372-4c98-aaf7-b96b8adbb59d"
   },
   "outputs": [],
   "source": [
    "my_output = torch.cat([style_image, face, my_sample], 0)\n",
    "display_image(utils.make_grid(my_output, normalize=True, range=(-1, 1)), title='My sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "4uL9cY9GN3Fa",
    "outputId": "27c78cbe-ef04-431f-ccfe-0e05d497c2df"
   },
   "outputs": [],
   "source": [
    "output = torch.cat([original_sample, sample], 0)\n",
    "display_image(utils.make_grid(output, normalize=True, range=(-1, 1), nrow=n_sample), title='Random samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNT9TWCioMC0"
   },
   "source": [
    "# Creating your own style\n",
    "- Copy your style images into the style_images Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3X3AT-vN6DH"
   },
   "outputs": [],
   "source": [
    "# #@markdown Upload your own style images into the style_images folder and type it into the field in the following format without the directory name. Upload multiple style images to do multi-shot image translation\n",
    "# names =  ['hulk-01.jpg', 'hulk-04.jpg']#@param {type:\"raw\"}\n",
    "# names =  ['drstrange-01.jpg', 'drstrange-02.jpg', 'drstrange-03.jpg', 'drstrange-04.jpg']#@param {type:\"raw\"}\n",
    "names =  ['sketch2.jpeg', 'sketch4.jpeg']#@param {type:\"raw\"}\n",
    "targets = []\n",
    "latents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZyLZRwhDJrc"
   },
   "source": [
    "## Latent Space Creation from the Style(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uizrcmPRoZiH"
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    style_path = os.path.join('style_images', name)\n",
    "    assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
    "\n",
    "    name = strip_path_extension(name)\n",
    "\n",
    "    # crop and align the face\n",
    "    style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
    "    if not os.path.exists(style_aligned_path):\n",
    "        style_aligned = align_face(style_path)\n",
    "        style_aligned.save(style_aligned_path)\n",
    "    else:\n",
    "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
    "\n",
    "    # GAN invert\n",
    "    style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
    "    if not os.path.exists(style_code_path):\n",
    "        latent = e4e_projection(style_aligned, style_code_path, device)\n",
    "    else:\n",
    "        latent = torch.load(style_code_path)['latent']\n",
    "\n",
    "    targets.append(transform(style_aligned).to(device))\n",
    "    latents.append(latent.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "2roOqMjgobke",
    "outputId": "c096279c-0d6e-4fbe-cd22-5ab739fef97f"
   },
   "outputs": [],
   "source": [
    "targets = torch.stack(targets, 0)\n",
    "latents = torch.stack(latents, 0)\n",
    "\n",
    "target_im = utils.make_grid(targets, normalize=True, range=(-1, 1))\n",
    "display_image(target_im, title='Style References')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfaxCAJmpwPV",
    "outputId": "2720c4b7-b07c-4072-aa14-440feae63c2f"
   },
   "outputs": [],
   "source": [
    "#@title Finetune StyleGAN\n",
    "#@markdown alpha controls the strength of the style\n",
    "alpha =  1.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "alpha = 1-alpha\n",
    "\n",
    "#@markdown Tries to preserve color of original image by limiting family of allowable transformations. Set to false if you want to transfer color from reference image. This also leads to heavier stylization\n",
    "preserve_color = False #@param{type:\"boolean\"}\n",
    "#@markdown Number of finetuning steps. Different style reference may require different iterations. Try 200~500 iterations.\n",
    "num_iter = 500 #@param {type:\"number\"}\n",
    "#@markdown Log training on wandb and interval for image logging\n",
    "use_wandb = False #@param {type:\"boolean\"}\n",
    "log_interval = 50 #@param {type:\"number\"}\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(project=\"JoJoGAN\")\n",
    "    config = wandb.config\n",
    "    config.num_iter = num_iter\n",
    "    config.preserve_color = preserve_color\n",
    "    wandb.log(\n",
    "    {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\n",
    "    step=0)\n",
    "\n",
    "# load discriminator for perceptual loss\n",
    "discriminator = Discriminator(1024, 2).eval().to(device)\n",
    "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
    "discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
    "\n",
    "# reset generator\n",
    "del generator\n",
    "generator = deepcopy(original_generator)\n",
    "\n",
    "g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n",
    "\n",
    "# Which layers to swap for generating a family of plausible real images -> fake image\n",
    "if preserve_color:\n",
    "    id_swap = [9,11,15,16,17]\n",
    "else:\n",
    "    id_swap = list(range(7, generator.n_latent))\n",
    "\n",
    "for idx in tqdm(range(num_iter)):\n",
    "    mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n",
    "    in_latent = latents.clone()\n",
    "    in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n",
    "\n",
    "    img = generator(in_latent, input_is_latent=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        real_feat = discriminator(targets)\n",
    "    fake_feat = discriminator(img)\n",
    "\n",
    "    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.log({\"loss\": loss}, step=idx)\n",
    "        if idx % log_interval == 0:\n",
    "            generator.eval()\n",
    "            my_sample = generator(my_w, input_is_latent=True)\n",
    "            generator.train()\n",
    "            my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n",
    "            wandb.log(\n",
    "            {\"Current stylization\": [wandb.Image(my_sample)]},\n",
    "            step=idx)\n",
    "\n",
    "    g_optim.zero_grad()\n",
    "    loss.backward()\n",
    "    g_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set PYTORCH_NO_CUDA_MEMORY_CACHING=1 \n",
    "!set 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsfWuvKep6FW",
    "outputId": "92bb5932-4568-4793-a5cf-2ff0aed7efe5"
   },
   "outputs": [],
   "source": [
    "#@title Generate results\n",
    "n_sample =  5#@param {type:\"number\"}\n",
    "seed = 3000 #@param {type:\"number\"}\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "with torch.no_grad():\n",
    "    generator.eval()\n",
    "    z = torch.randn(n_sample, latent_dim, device=device)\n",
    "\n",
    "    #original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
    "    #sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
    "\n",
    "    #original_my_sample = original_generator(my_w, input_is_latent=True)\n",
    "    my_sample = generator(my_w, input_is_latent=True)\n",
    "\n",
    "# display reference images\n",
    "style_images = []\n",
    "for name in names:\n",
    "    style_path = f'style_images_aligned/{strip_path_extension(name)}.png'\n",
    "    style_image = transform(Image.open(style_path))\n",
    "    style_images.append(style_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_VQinFkqMgK"
   },
   "source": [
    "# Saving the model checkpoint to the Local Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_generator.state_dict()\n",
    "torch.save(original_generator.state_dict(), '/content/JoJoGAN/models/lightyear-suite-chk.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_VQinFkqMgK"
   },
   "source": [
    "## Display Reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "cBrZrX2oqGzX",
    "outputId": "beb5f812-e2f3-4e7a-bae1-05087b5727e4"
   },
   "outputs": [],
   "source": [
    "face = transform(aligned_face).to(device).unsqueeze(0)\n",
    "style_images = torch.stack(style_images, 0).to(device)\n",
    "#display_image(utils.make_grid(style_images, normalize=True, range=(-1, 1)), title='References')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0MTc97QqPX4"
   },
   "source": [
    "## Display Transformed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "Bh3luvpSqIUF",
    "outputId": "3413a02c-0eb4-4436-a5f2-15a68b86134a"
   },
   "outputs": [],
   "source": [
    "my_output = torch.cat([style_images, face, my_sample], 0)\n",
    "display_image(utils.make_grid(my_output, normalize=True, range=(-1, 1)), title='My sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn-l0Y1XqRYh"
   },
   "source": [
    "## Display Transformed images using Random Faces from the Faces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "YEfAxRHwqJQw",
    "outputId": "286647e3-f050-42b8-af56-ba88b7ce0b05"
   },
   "outputs": [],
   "source": [
    "output = torch.cat([original_sample, sample], 0)\n",
    "display_image(utils.make_grid(output, normalize=True, range=(-1, 1), nrow=n_sample), title='Random samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVtVBpljsP5_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JoJoGAN-Exercise.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "jojo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d57b74376ffde15dfb451e7533dfd55ff26d30b9206d524a4e05c5dfaf450fa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
